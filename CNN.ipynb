{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c50f3e3ac5d09917dcad0bd27fa8129661f10df2"
   },
   "source": [
    "# Fashion MNIST - 3D CNN using Tensorflow,Keras\n",
    "\n",
    "*  The objective of this kernel is to define,compile & evaluate a 3 layer convolutional neural network(CNN) model \n",
    "\n",
    "*  Visualise the validation accuracy and validation loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd7ed739f7cfd6090fdc7af39c6ff006fa725a9b"
   },
   "source": [
    "**Create dataframes for train and test datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./input_data/train.csv',sep=',')\n",
    "test_df = pd.read_csv('./input_data/test.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24b57c3c74765954d022767beafe991913c4eea3"
   },
   "source": [
    "Now let us split the train data into x and y arrays where x represents the image data and y represents the labels. To do that we need to convert the dataframes into numpy arrays of float32 type which is the acceptable form for tensorflow and keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_df, dtype = 'float32')\n",
    "test_data = np.array(test_df, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us slice the train arrays into x and y arrays namely x_train,y_train to store all image data and label data respectively. i.e\n",
    "\n",
    "x_train contains all the rows and all columns except the label column and excluding header info .\n",
    "y_train contains all the rows and first column and excluding header info .\n",
    "Similarly slice the test arrays into x and y arrays namely x_train,y_train to store all image data and label data respectively. i.e\n",
    "\n",
    "x_test contains all the rows and all columns except the label column and excluding header info .\n",
    "y_test contains all the rows and first column and excluding header info . #### Important Note : Since the image data in x_train and x_test is from 0 to 255 , we need to rescale this from 0 to 1.To do this we need to divide the x_train and x_test by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data[:,1:]/255\n",
    "y_train = train_data[:,0]\n",
    "x_test= test_data[:,1:]/255\n",
    "y_test=test_data[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are gonna split the training data into validation and actual training data for training the model and testing it using the validation set. This is achieved using the train_test_split method of scikit learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0e59d45dffbb07f4d540740ebf5e83b787d5029"
   },
   "source": [
    "\n",
    "Now let us visualise the sample image how it looks like in 28 * 28 pixel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2853b89c2ddcf650dc9afbd71a5aed011f2ff6d4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = x_train[13,:].reshape((28,28))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f946ddc4bbd4286e59540af2deb2eeabcdfe5bf1"
   },
   "source": [
    "As you can observe above the shape of shoe from the sample image\n",
    "\n",
    "### Create the 3D Convolutional Neural Networks (CNN)\n",
    "\n",
    "- #### Define the model\n",
    "- #### Compile the model\n",
    "- #### Fit the model\n",
    "\n",
    "First of all let us define the shape of the image before we define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "234bde019b14ac2b90aac9021d2f52b206b9637d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_rows = 28\n",
    "image_cols = 28\n",
    "batch_size = 512\n",
    "image_shape = (image_rows,image_cols,1) # Defined the shape of the image as 3d with rows and columns and 1 for the 3d visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ead99bd967b2f20bd20ea2ba3ff4d9fe1dff660"
   },
   "source": [
    "Now we need to do more formating on the x_train,x_test and x_validate sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f6f09509b632b4793b94c124beaa570f615d6f30",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],*image_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0],*image_shape)\n",
    "x_validate = x_validate.reshape(x_validate.shape[0],*image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a6be86935dbdcbd2e92b379bd076ddf692db3c6f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"x_train shape = {}\".format(x_train.shape))\n",
    "print(\"x_test shape = {}\".format(x_test.shape))\n",
    "print(\"x_validate shape = {}\".format(x_validate.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dadda40bc9dc6a993a89755f25140d42e674be9b"
   },
   "source": [
    "#### Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "781f5874afaeb361a9743d0c37dec9636f3c507b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = '1_Layer'\n",
    "cnn_model_1 = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, name='Conv2D-1'),\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool'),\n",
    "    Dropout(0.2, name='Dropout'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(32, activation='relu', name='Dense'),\n",
    "    Dense(10, activation='softmax', name='Output')\n",
    "], name=name)\n",
    "\n",
    "name = '2_Layer'\n",
    "cnn_model_2 = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, name='Conv2D-1'),\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool'),\n",
    "    Dropout(0.2, name='Dropout-1'),\n",
    "    Conv2D(64, kernel_size=3, activation='relu', name='Conv2D-2'),\n",
    "    Dropout(0.25, name='Dropout-2'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(64, activation='relu', name='Dense'),\n",
    "    Dense(10, activation='softmax', name='Output')\n",
    "], name=name)\n",
    "\n",
    "name='3_layer'\n",
    "cnn_model_3 = Sequential([\n",
    "    Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, name='Conv2D-1'),\n",
    "    MaxPooling2D(pool_size=2, name='MaxPool'),\n",
    "    Dropout(0.25, name='Dropout-1'),\n",
    "    Conv2D(64, kernel_size=3, activation='relu', name='Conv2D-2'),\n",
    "    Dropout(0.25, name='Dropout-2'),\n",
    "    Conv2D(128, kernel_size=3, activation='relu', name='Conv2D-3'),\n",
    "    Dropout(0.4, name='Dropout-3'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(128, activation='relu', name='Dense'),\n",
    "    Dropout(0.4, name='Dropout'),\n",
    "    Dense(10, activation='softmax', name='Output')\n",
    "], name=name)\n",
    "\n",
    "cnn_models = [cnn_model_1, cnn_model_2, cnn_model_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63f352eca83472d32789f725c909049d15e76f9f"
   },
   "source": [
    "**the model summaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "153e50645ce1c1c52e19bfb5fe5fc458f92d6c51",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in cnn_models:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "795cf81b9d18340f49bf9e483b326929f290f23d"
   },
   "source": [
    " **train the models and save results to a dict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e66a324af7495c60223db4a266dd2f54f4f722d7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_dict = {}\n",
    "for model in cnn_models:\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit( x_train, y_train, batch_size=batch_size, epochs=50, verbose=1, validation_data=(x_validate, y_validate))\n",
    "    \n",
    "    history_dict[model.name] = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c571b2bd7975a29ecad8a6ba6c1eac2258b519d"
   },
   "source": [
    "### Plot the Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b18c4b492c3d5e864a31783ddb5c8029661f455c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(2,figsize=(8,6))\n",
    "for history in history_dict:\n",
    "    val_acc = history_dict[history].history['val_accuracy']\n",
    "    val_loss = history_dict[history].history['val_loss']\n",
    "    ax1.plot(val_acc, label=history)\n",
    "    ax2.plot(val_loss, label=history)\n",
    "ax1.set_ylabel('Validation Accuracy')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6bfc7a1a12478c0adeff5b811638299361b42920",
    "collapsed": true
   },
   "source": [
    "As you can see in the above graph as the no of convolution layers increases the accuracy is increasing and loss keep decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(cnn_models):\n",
    "\tmodel.save(fr'./saved_models/cnn_{i}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rows = 28\n",
    "image_cols = 28\n",
    "batch_size = 512\n",
    "image_shape = (image_rows,image_cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [keras.models.load_model(file.path) for file in os.scandir('./saved_models') if 'cnn' in file.path]\n",
    "\n",
    "train_df = pd.read_csv('./input_data/train.csv',sep=',')\n",
    "test_df = pd.read_csv('./input_data/test.csv', sep = ',')\n",
    "\n",
    "train_data = np.array(train_df, dtype = 'float32')\n",
    "test_data = np.array(test_df, dtype='float32')\n",
    "\n",
    "x_train = train_data[:,1:]/255\n",
    "y_train = train_data[:,0]\n",
    "x_test= test_data[:,1:]/255\n",
    "y_test=test_data[:,0]\n",
    "\n",
    "x_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 12345)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],*image_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0],*image_shape)\n",
    "x_validate = x_validate.reshape(x_validate.shape[0],*image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the line profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lp_cnn_1(model, x):\n",
    "    x = model.layers[0](x) # Conv2D\n",
    "    x = model.layers[1](x) # MaxPooling2D\n",
    "    x = model.layers[2](x) # Dropout\n",
    "    x = model.layers[3](x) # Flatten\n",
    "    x = model.layers[4](x) # Dense (relu)\n",
    "    x = model.layers[5](x) # Dense (softmax)\n",
    "\n",
    "def lp_cnn_2(model, x):\n",
    "    x = model.layers[0](x) # Conv2D\n",
    "    x = model.layers[1](x) # MaxPooling2D\n",
    "    x = model.layers[2](x) # Dropout\n",
    "    x = model.layers[3](x) # Conv2D\n",
    "    x = model.layers[4](x) # Dropout\n",
    "    x = model.layers[5](x) # Flatten\n",
    "    x = model.layers[6](x) # Dense (relu)\n",
    "    x = model.layers[7](x) # Dense (softmax)\n",
    "\n",
    "def lp_cnn_3(model, x):\n",
    "    x = model.layers[0](x) # Conv2D\n",
    "    x = model.layers[1](x) # MaxPooling2D\n",
    "    x = model.layers[2](x) # Dropout\n",
    "    x = model.layers[3](x) # Conv2D\n",
    "    x = model.layers[4](x) # Dropout\n",
    "    x = model.layers[5](x) # Conv2D\n",
    "    x = model.layers[6](x) # Dropout\n",
    "    x = model.layers[7](x) # Flatten\n",
    "    x = model.layers[8](x) # Dens (relu)\n",
    "    x = model.layers[9](x) # Dropout\n",
    "    x = model.layers[10](x) # Dense (softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profile_reader import read_lprun, read_mprun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the line profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -T ./profiles/lp/cnn_layer1.txt -f lp_cnn_1 lp_cnn_1(models[0], np.float_(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -T ./profiles/lp/cnn_layer2.txt -f lp_cnn_2 lp_cnn_2(models[1], np.float_(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -T ./profiles/lp/cnn_layer3.txt -f lp_cnn_3 lp_cnn_3(models[2], np.float_(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "read_lprun('cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the memory profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the function profilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file mp_cnn.py\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "from memory_profiler import profile\n",
    "\n",
    "@profile(precision=10)\n",
    "def mp_cnn_1(image_shape):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, name='Conv2D-1'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=2, name='MaxPool'))\n",
    "\tmodel.add(Dropout(0.2, name='Dropout'))\n",
    "\tmodel.add(Flatten(name='flatten'))\n",
    "\tmodel.add(Dense(32, activation='relu', name='Dense'))\n",
    "\tmodel.add(Dense(10, activation='softmax', name='Output'))\n",
    "\n",
    "@profile(precision=10)\n",
    "def mp_cnn_2(image_shape):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, name='Conv2D-1'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=2, name='MaxPool'))\n",
    "\tmodel.add(Dropout(0.2, name='Dropout-1'))\n",
    "\tmodel.add(Conv2D(64, kernel_size=3, activation='relu', name='Conv2D-2'))\n",
    "\tmodel.add(Dropout(0.25, name='Dropout-2'))\n",
    "\tmodel.add(Flatten(name='flatten'))\n",
    "\tmodel.add(Dense(64, activation='relu', name='Dense'))\n",
    "\tmodel.add(Dense(10, activation='softmax', name='Output'))\n",
    "\n",
    "@profile(precision=10)\n",
    "def mp_cnn_3(image_shape):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=image_shape, kernel_initializer='he_normal', name='Conv2D-1'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=2, name='MaxPool'))\n",
    "\tmodel.add(Dropout(0.25, name='Dropout-1'))\n",
    "\tmodel.add(Conv2D(64, kernel_size=3, activation='relu', name='Conv2D-2'))\n",
    "\tmodel.add(Dropout(0.25, name='Dropout-2'))\n",
    "\tmodel.add(Conv2D(128, kernel_size=3, activation='relu', name='Conv2D-3'))\n",
    "\tmodel.add(Dropout(0.4, name='Dropout-3'))\n",
    "\tmodel.add(Flatten(name='flatten'))\n",
    "\tmodel.add(Dense(128, activation='relu', name='Dense'))\n",
    "\tmodel.add(Dropout(0.4, name='Dropout'))\n",
    "\tmodel.add(Dense(10, activation='softmax', name='Output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp_cnn import mp_cnn_1, mp_cnn_2, mp_cnn_3\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout = sys.stdout\n",
    "sys.stdout = open('./profiles/mp/cnn_layer1.txt', 'w')\n",
    "mp_cnn_1(image_shape)\n",
    "sys.stdout = open('./profiles/mp/cnn_layer2.txt', 'w')\n",
    "mp_cnn_2(image_shape)\n",
    "sys.stdout = open('./profiles/mp/cnn_layer3.txt', 'w')\n",
    "mp_cnn_3(image_shape)\n",
    "sys.stdout = stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_mprun('cnn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
